{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Training Data:</b>\n",
    "\n",
    "POS-tagged data from Berkeley Restaurant corpus. ~15,000 sentences in corpus. \n",
    "\n",
    "Assume: 1) POS tagset is closed. 2) New words will occur in testset. \n",
    "\n",
    "File format: Sentences are arranged as 1 word per line with blank line separating the sentences. Columns are tab separated. 1st col is word position, 2nd col is word, and 3rd col is POS tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fname = 'berp-POS-training.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\ti\\tPRP\\n',\n",
       " \"2\\t'd\\tMD\\n\",\n",
       " '3\\tlike\\tVB\\n',\n",
       " '4\\tto\\tTO\\n',\n",
       " '5\\tgo\\tVB\\n',\n",
       " '6\\tto\\tIN\\n',\n",
       " '7\\ta\\tDT\\n',\n",
       " '8\\tfancy\\tJJ\\n',\n",
       " '9\\trestaurant\\tNN\\n',\n",
       " '10\\t.\\t.\\n',\n",
       " '\\n',\n",
       " '1\\ti\\tPRP\\n',\n",
       " \"2\\t'd\\tMD\\n\",\n",
       " '3\\tlike\\tVB\\n',\n",
       " '4\\tfrench\\tJJ\\n',\n",
       " '5\\tfood\\tNN\\n',\n",
       " '6\\t.\\t.\\n',\n",
       " '\\n',\n",
       " '1\\tnext\\tJJ\\n',\n",
       " '2\\tthursday\\tNN\\n']"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_fname, 'r') as file:\n",
    "    training_data = file.readlines()\n",
    "    \n",
    "# First few sentences & the space after them\n",
    "training_data[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Evaluation:</b>\n",
    "\n",
    "Basic script is provided and calculates overall accuracy compared to a gold standard eval set. \n",
    "\n",
    "``` python eval-pos.py  gold-file system-file ```\n",
    "\n",
    "Produce a confusion matrix for more useful tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Task: Build a probabilistic tagger</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>1) Baseline system:</b> \n",
    "Implement a \"most frequent tag\" system. Given counts from training data, the tagger should simply assign to each input word the tag that it was most frequently assigned to in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pos_count(training_data):\n",
    "\n",
    "    # Given a training set, create a list of lists as lookup item\n",
    "    # Each inside list is a word, its POS, and the frequency of this pairing\n",
    "\n",
    "    # Create list of tuple: (word, POS)\n",
    "    tups_list = []\n",
    "    for line in training_data:\n",
    "        if line != '\\n':\n",
    "            split_line = line.strip().split('\\t')\n",
    "            word_pos = split_line[1], split_line[2]\n",
    "            tups_list.append(word_pos)\n",
    "\n",
    "    # Get count of each unique tuple\n",
    "    count_set = dict((x, tups_list.count(x)) for x in set(tups_list))\n",
    "\n",
    "    # Create list of [word, POS, count]\n",
    "    word_pos_count = []\n",
    "    for k in count_set.keys():\n",
    "        k_list = list(k)\n",
    "        k_list.append(count_set[k])\n",
    "        word_pos_count.append(k_list)\n",
    "    \n",
    "    return word_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 lists of lookup item\n",
    "word_pos_count(training_data)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['casa-de-eva', 'NN', 21], ['casa-de-eva', 'NNP', 2]] \n",
      "\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "input_word = 'casa-de-eva'\n",
    "\n",
    "# Matching input word to a word-tag-count\n",
    "matching_l = []\n",
    "for l in word_pos_count:    \n",
    "    if l[0] == input_word:\n",
    "        matching_l.append(l)\n",
    "    \n",
    "print('{} \\n'.format(matching_l))\n",
    "\n",
    "\n",
    "most_frequent_tag = ''\n",
    "# Dealing with unseen words, assign POS 'UNK'\n",
    "if len(matching_l) == 0:\n",
    "    most_frequent_tag = 'UNK'\n",
    "    \n",
    "else:\n",
    "    # Find max count and tag that POS to word\n",
    "    max_count = matching_l[0][2]\n",
    "    for match in matching_l:\n",
    "        if match[2] > max_count:\n",
    "            most_frequent_tag = match[1]\n",
    "        elif match[2] == max_count:\n",
    "            most_frequent_tag = match[1]\n",
    "\n",
    "print(most_frequent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on a test file\n",
    "with open('testfile.txt', 'w') as the_file:\n",
    "    pass # Empty content before writing\n",
    "    the_file.write(\"1\\ti\\n\")\n",
    "    the_file.write(\"2\\t'd\\n\")\n",
    "    the_file.write(\"3\\tlike\\n\")\n",
    "    the_file.write(\"4\\tzachary\\n\")\n",
    "    the_file.write(\"5\\t's\\n\")\n",
    "    the_file.write(\"6\\ta\\n\")\n",
    "    the_file.write(\"7\\t-\\n\")\n",
    "    the_file.write(\"8\\tla\\n\")\n",
    "    the_file.write(\"9\\t-\\n\")\n",
    "    the_file.write(\"10\\tcarte\\n\")\n",
    "    the_file.write(\"11\\t.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize needed?\n",
    "# input_sentence = \"i'd i've zachary's a-la-carte at 11 am.\"\n",
    "\n",
    "# split_sentence = word_tokenize(input_sentence)\n",
    "# print(split_sentence)\n",
    "\n",
    "# for token_i, token in enumerate(split_sentence):\n",
    "#     if ('-' in token):\n",
    "#         split_token = re.split('(\\W)', token)\n",
    "#         print(split_token)\n",
    "#         split_sentence[token_i] = split_token\n",
    "        \n",
    "# list(np.hstack(split_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
