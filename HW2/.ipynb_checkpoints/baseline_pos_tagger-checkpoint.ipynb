{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: for unseen words, baseline should output most frequent tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Training Data:</b>\n",
    "\n",
    "POS-tagged data from Berkeley Restaurant corpus. ~15,000 sentences in corpus. \n",
    "\n",
    "Assume: 1) POS tagset is closed. 2) New words will occur in testset. \n",
    "\n",
    "File format: Sentences are arranged as 1 word per line with blank line separating the sentences. Columns are tab separated. 1st col is word position, 2nd col is word, and 3rd col is POS tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fname = 'training_set_shuffled.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_fname, 'r') as file:\n",
    "    training_data = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Evaluation:</b>\n",
    "\n",
    "Basic script is provided and calculates overall accuracy compared to a gold standard eval set. \n",
    "\n",
    "``` python eval-pos.py  gold-file system-file ```\n",
    "\n",
    "Produce a confusion matrix for more useful tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Task: Build a probabilistic tagger</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>1) Baseline system:</b> \n",
    "Implement a \"most frequent tag\" system. Given counts from training data, the tagger should simply assign to each input word the tag that it was most frequently assigned to in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_count(training_data):\n",
    "    '''\n",
    "    Given a training set, create a list of lists as lookup table.\n",
    "    Each inside list is a word, its POS, and the frequency of this pairing.\n",
    "    '''\n",
    "    \n",
    "    # Create list of tuple: (word, POS)\n",
    "    tups_list = []\n",
    "    for line in training_data:\n",
    "        if line != '\\n':\n",
    "            split_line = line.strip().split('\\t')\n",
    "            word_pos = split_line[1], split_line[2]\n",
    "            tups_list.append(word_pos)\n",
    "\n",
    "    # Get count of each unique tuple\n",
    "    count_set = dict((x, tups_list.count(x)) for x in set(tups_list))\n",
    "\n",
    "    # Create list of [word, POS, count]\n",
    "    word_pos_count = []\n",
    "    for k in count_set.keys():\n",
    "        k_list = list(k)\n",
    "        k_list.append(count_set[k])\n",
    "        word_pos_count.append(k_list)\n",
    "    \n",
    "    return word_pos_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_tag(word_pos_count, input_word):\n",
    "    '''\n",
    "    Given word_pos_count from training data & an input word, \n",
    "    get the most frequent POS by \n",
    "    calling the function get_word_pos_count() to get the lookup table\n",
    "    and match input word to most frequent POS.\n",
    "    \n",
    "    Return most frequent tag for that word.\n",
    "    '''\n",
    "    \n",
    "    # Get word_pos_count lookup table\n",
    "    # word_pos_count = get_word_pos_count(training_data)\n",
    "    \n",
    "    # Get max word_pos_count's tag\n",
    "    counts = []\n",
    "    for i, word_pos in enumerate(word_pos_count):\n",
    "        counts.append(word_pos_count[i][2])\n",
    "    most_freq = word_pos_count[np.argmax(counts)][1]\n",
    "    \n",
    "    # Matching input word to possible word-tag-count lists\n",
    "    matching_l = []\n",
    "    for l in word_pos_count:    \n",
    "        if l[0] == input_word:\n",
    "            matching_l.append(l)\n",
    "\n",
    "    # print('{} \\n'.format(matching_l))\n",
    "\n",
    "    # Find most frequent POS tag\n",
    "    most_frequent_tag = ''\n",
    "    # Dealing with unseen words (matching_l: []), assign POS 'UNK'\n",
    "    if len(matching_l) == 0:\n",
    "        most_frequent_tag = most_freq\n",
    "    else:\n",
    "        # Find max count and tag that POS to word\n",
    "        max_count = matching_l[0][2] # Temporary max\n",
    "        for match in matching_l:\n",
    "            if match[2] > max_count:\n",
    "                most_frequent_tag = match[1]\n",
    "            elif match[2] == max_count:\n",
    "                most_frequent_tag = match[1]\n",
    "                \n",
    "    return most_frequent_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function\n",
    "input_word = 'food'\n",
    "get_most_frequent_tag(word_pos_count, input_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the short test file\n",
    "with open('test_set_shuffled.txt', 'r') as test_file:\n",
    "    test_data = test_file.readlines()\n",
    "\n",
    "baseline_output = []\n",
    "for line in test_data:\n",
    "    if line != \"\\n\":\n",
    "        line = line.split()\n",
    "        word = line[1]\n",
    "        tag = get_most_frequent_tag(word_pos_count, word)\n",
    "        baseline_output.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = ''\n",
    "for line in test_data:\n",
    "    if line != '\\n':\n",
    "        split_line = line.strip().split('\\t')\n",
    "        word = split_line[1]\n",
    "        tag = get_most_frequent_tag(word_pos_count, word)\n",
    "        # print(word, tag)\n",
    "        new_file += '{}\\t{}\\t{}\\n'.format(split_line[0], word, tag)\n",
    "    elif line == '\\n':\n",
    "        new_file += '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new_file with POs tags to a file\n",
    "with open('baseline_test.txt', 'w') as baseline:\n",
    "    pass # Empty content before writing\n",
    "    baseline.write(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>2) Viterbi algorithm:</b>  \n",
    "\n",
    "Implement Viterbi with a bigram-based approach (only need previous to infer current). \n",
    "1. Extract required counts from training data to generate required probability estimates for model.\n",
    "2. Deal with unknown words in some sensible way: UNK\n",
    "3. Do some form of smoothing for the bigram tag model: Add 1\n",
    "4. Implement Viterbi decoder.\n",
    "5. Evaluate performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Step 1. Create state transition probability matrix</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\ti\\tPRP\\n'"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_train = training_data[0:7]\n",
    "short_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PRP'], 1, ['PRP', 'MD', 'VB', 'TO', 'VB', 'IN', 'DT'])"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ordered list of POS & observed tokens\n",
    "pos_array = []\n",
    "token_array = []\n",
    "\n",
    "# Get list of POS that are at position 1 in sentences\n",
    "init_array = []\n",
    "\n",
    "num_sentences = 0\n",
    "\n",
    "for line in short_train:\n",
    "    if line != '\\n':\n",
    "        split_line = line.strip().split('\\t')\n",
    "        pos = split_line[2]\n",
    "        token = split_line[1]\n",
    "        pos_array.append(pos)\n",
    "        token_array.append(token)\n",
    "        \n",
    "        if split_line[0] == '1':\n",
    "            init_array.append(split_line[2])\n",
    "            num_sentences += 1\n",
    "            \n",
    "init_array, num_sentences, pos_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>followed:</th>\n",
       "      <th>DT</th>\n",
       "      <th>IN</th>\n",
       "      <th>MD</th>\n",
       "      <th>PRP</th>\n",
       "      <th>TO</th>\n",
       "      <th>VB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>given:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "followed:   DT   IN   MD  PRP   TO   VB\n",
       "given:                                 \n",
       "DT         1.0  0.0  0.0  0.0  0.0  0.0\n",
       "IN         0.0  1.0  0.0  0.0  0.0  0.0\n",
       "MD         0.0  0.0  1.0  0.0  0.0  0.0\n",
       "PRP        0.0  0.0  0.0  1.0  0.0  0.0\n",
       "TO         0.0  0.0  0.0  0.0  1.0  0.0\n",
       "VB         0.0  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix = pd.crosstab(pd.Series(pos_array[:], name='given:'),\n",
    "                                pd.Series(pos_array[:], name='followed:'), \n",
    "                                normalize=0)\n",
    "\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-775-f7d368e665f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-775-f7d368e665f6>\u001b[0m in \u001b[0;36mtransition_matrix\u001b[0;34m(transitions)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#now convert to probabilities:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def transition_matrix(transitions):\n",
    "    n = len(set(transitions)) #number of states\n",
    "\n",
    "    M = [[0]*n for _ in range(n)]\n",
    "\n",
    "    for (i,j) in zip(transitions,transitions[1:]):\n",
    "        M[i][j] += 1\n",
    "\n",
    "    #now convert to probabilities:\n",
    "    for row in M:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    return M\n",
    "\n",
    "m = transition_matrix(pos_array)\n",
    "for row in m: print(' '.join('{0:.2f}'.format(x) for x in row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_matrix_np = transition_matrix.reset_index().values\n",
    "# transition_matrix_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Step 2. Create emission / observation likelihood matrix</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:   'd    a   go    i  like   to\n",
      "tag:                                \n",
      "DT     0.0  1.0  0.0  0.0   0.0  0.0\n",
      "IN     0.0  0.0  0.0  0.0   0.0  1.0\n",
      "MD     1.0  0.0  0.0  0.0   0.0  0.0\n",
      "PRP    0.0  0.0  0.0  1.0   0.0  0.0\n",
      "TO     0.0  0.0  0.0  0.0   0.0  1.0\n",
      "VB     0.0  0.0  0.5  0.0   0.5  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_matrix = pd.crosstab(pd.Series(pos_array[:], name=\"tag:\"),\n",
    "                              pd.Series(token_array, name=\"word:\"), \n",
    "                              normalize=0)\n",
    "print(emission_matrix)\n",
    "emission_matrix.at['VB','go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_matrix_np = emission_matrix.reset_index().values\n",
    "# emission_matrix_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\***** smooth with Laplace add-1 smoothing for emission probs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Step 3. Create initial probability vector: prob of a POS follwing start of sentence</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MD': 0.0, 'PRP': 1.0, 'TO': 0.0, 'VB': 0.0}"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_vector = {}\n",
    "for pos in list(set(pos_array)):\n",
    "    init_vector[pos] = init_array.count(pos) / num_sentences\n",
    "init_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Step 3. Create table where columns are observations (all sentences in order) and rows are possible hidden states. \n",
    "    \n",
    "Step 4. Sweep through table and find max prob and path. </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "NN\n",
      "PRP\n",
      ".\n",
      "TO\n",
      "MD\n",
      "IN\n",
      "VB\n",
      "JJ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.66666667, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = ['i', \"'d\", 'like', 'food'] # emissions/seq - aka x\n",
    "Q =  list(set(pos_array)) # set of states\n",
    "'''\n",
    "Given a sequence of emissions, return the most probably path and \n",
    "its joint probability.\n",
    "'''\n",
    "\n",
    "nrow, ncol = len(Q), len(x)+1\n",
    "mat = np.zeros(shape=(nrow, ncol), dtype=float) # prob table\n",
    "matTb = np.zeros(shape=(nrow, ncol), dtype=int) # backtrace\n",
    "\n",
    "# Fill in 1st column of mat: P(POS|start) * P(word|POS)\n",
    "start_col = []\n",
    "for i, pos in enumerate(Q):\n",
    "    print(pos)\n",
    "    mat[i, 0] = init_vector[pos] * emission_matrix.at[pos, input_seq[0]]\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Fill in the rest of mat table\n",
    "for j, token in enumerate(input_seq):\n",
    "#     print(j, token)    \n",
    "    one_token_all_states = []\n",
    "    for i, pos in enumerate(Q):\n",
    "        # Probability of first word \"i\" at each pos = mat[i, 0] * P(word|POS)\n",
    "        # Starts filling in mat at j=1 now\n",
    "        \n",
    "        one_token_all_states.append(start_col[i] * emission_matrix.at[pos, token])\n",
    "        \n",
    "        # mat[i, j+1] = mat[i, j] * emission_matrix.at[pos, token]\n",
    "#         for i2, pos2 in enumerate(Q):\n",
    "            \n",
    "#     * emission_matrix.at[pos, x[i]]\n",
    "\n",
    "#     emission_matrix[i, x[0]] * init_vector[i]\n",
    "    print(one_token_all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on a test file\n",
    "with open('testfile.txt', 'w') as the_file:\n",
    "    pass # Empty content before writing\n",
    "    the_file.write(\"1\\ti\\n\")\n",
    "    the_file.write(\"2\\t'd\\n\")\n",
    "    the_file.write(\"3\\tlike\\n\")\n",
    "    the_file.write(\"4\\tzachary\\n\")\n",
    "    the_file.write(\"5\\t's\\n\")\n",
    "    the_file.write(\"6\\ta\\n\")\n",
    "    the_file.write(\"7\\t-\\n\")\n",
    "    the_file.write(\"8\\tla\\n\")\n",
    "    the_file.write(\"9\\t-\\n\")\n",
    "    the_file.write(\"10\\tcarte\\n\")\n",
    "    the_file.write(\"11\\t.\\n\")\n",
    "    the_file.write('\\n')\n",
    "    the_file.write(\"1\\ti\\n\")\n",
    "    the_file.write(\"2\\tunseen\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize needed?\n",
    "# input_sentence = \"i'd i've zachary's a-la-carte at 11 am.\"\n",
    "\n",
    "# split_sentence = word_tokenize(input_sentence)\n",
    "# print(split_sentence)\n",
    "\n",
    "# for token_i, token in enumerate(split_sentence):\n",
    "#     if ('-' in token):\n",
    "#         split_token = re.split('(\\W)', token)\n",
    "#         print(split_token)\n",
    "#         split_sentence[token_i] = split_token\n",
    "        \n",
    "# list(np.hstack(split_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
