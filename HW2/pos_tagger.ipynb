{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dieumynguyen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Training Data:</b>\n",
    "\n",
    "POS-tagged data from Berkeley Restaurant corpus. ~15,000 sentences in corpus. \n",
    "\n",
    "Assume: 1) POS tagset is closed. 2) New words will occur in testset. \n",
    "\n",
    "File format: Sentences are arranged as 1 word per line with blank line separating the sentences. Columns are tab separated. 1st col is word position, 2nd col is word, and 3rd col is POS tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fname = 'berp-POS-training.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\ti\\tPRP\\n',\n",
       " \"2\\t'd\\tMD\\n\",\n",
       " '3\\tlike\\tVB\\n',\n",
       " '4\\tto\\tTO\\n',\n",
       " '5\\tgo\\tVB\\n',\n",
       " '6\\tto\\tIN\\n',\n",
       " '7\\ta\\tDT\\n',\n",
       " '8\\tfancy\\tJJ\\n',\n",
       " '9\\trestaurant\\tNN\\n',\n",
       " '10\\t.\\t.\\n',\n",
       " '\\n',\n",
       " '1\\ti\\tPRP\\n',\n",
       " \"2\\t'd\\tMD\\n\",\n",
       " '3\\tlike\\tVB\\n',\n",
       " '4\\tfrench\\tJJ\\n',\n",
       " '5\\tfood\\tNN\\n',\n",
       " '6\\t.\\t.\\n',\n",
       " '\\n',\n",
       " '1\\tnext\\tJJ\\n',\n",
       " '2\\tthursday\\tNN\\n',\n",
       " '3\\t.\\t.\\n']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_fname, 'r') as file:\n",
    "    training_data = file.readlines()\n",
    "    \n",
    "# First few sentences & the space after them\n",
    "training_data[0:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Evaluation:</b>\n",
    "\n",
    "Basic script is provided and calculates overall accuracy compared to a gold standard eval set. \n",
    "\n",
    "``` python eval-pos.py  gold-file system-file ```\n",
    "\n",
    "Produce a confusion matrix for more useful tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Task: Build a probabilistic tagger</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>1) Baseline system:</b> \n",
    "Implement a \"most frequent tag\" system. Given counts from training data, the tagger should simply assign to each input word the tag that it was most frequently assigned to in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuple: (word, POS)\n",
    "tups_list = []\n",
    "for line in training_data[:]:\n",
    "    if line != '\\n':\n",
    "        split_line = line.strip().split('\\t')\n",
    "        word_pos = split_line[1], split_line[2]\n",
    "        tups_list.append(word_pos)\n",
    "\n",
    "# Get count of each unique tuple\n",
    "count_set = dict((x, tups_list.count(x)) for x in set(tups_list))\n",
    "\n",
    "# Create list of [word, POS, count]\n",
    "word_pos_count = []\n",
    "for k in count_set.keys():\n",
    "    k_list = list(k)\n",
    "    k_list.append(count_set[k])\n",
    "    word_pos_count.append(k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'s\", 'POS', 1113], [\"'s\", 'PRP', 426], [\"'s\", 'VBZ', 478]] \n",
      "\n",
      "POS\n"
     ]
    }
   ],
   "source": [
    "input_word = '\\'s'\n",
    "\n",
    "# Matching input word to a word-tag-count\n",
    "matching_l = []\n",
    "for l in word_pos_count:    \n",
    "    if l[0] == input_word:\n",
    "        matching_l.append(l)\n",
    "    \n",
    "print('{} \\n'.format(matching_l))\n",
    "\n",
    "\n",
    "most_frequent_tag = ''\n",
    "# Dealing with unseen words, assign POS 'UNK'\n",
    "if len(matching_l) == 0:\n",
    "    most_frequent_tag = 'UNK'\n",
    "    \n",
    "else:\n",
    "    # Find max count and tag that POS to word\n",
    "    max_count = matching_l[0][2]\n",
    "    for match in matching_l:\n",
    "        if match[2] > max_count:\n",
    "            most_frequent_tag = match[1]\n",
    "        elif match[2] == max_count:\n",
    "            most_frequent_tag = match[1]\n",
    "\n",
    "print(most_frequent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"i'd zachary's a-la-carte.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"'\",\n",
       " 'd',\n",
       " ' ',\n",
       " 'zachary',\n",
       " \"'\",\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " '-',\n",
       " 'la',\n",
       " '-',\n",
       " 'carte',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('(\\W)', input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'d\", 'zachary', \"'s\", 'a-la-carte', '.']\n",
      "['a', '-', 'la', '-', 'carte']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', \"'d\", 'zachary', \"'s\", 'a', '-', 'la', '-', 'carte', '.']"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentence = word_tokenize(input_sentence)\n",
    "print(split_sentence)\n",
    "\n",
    "for token_i, token in enumerate(split_sentence):\n",
    "    if '-' in token:\n",
    "        split_token = re.split('(\\W)', token)\n",
    "        print(split_token)\n",
    "        split_sentence[token_i] = split_token\n",
    "        \n",
    "list(np.hstack(split_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
